{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CS224N 2019-20: Homework 3\n",
    "parser_model.py: Feed-Forward Neural Network for Dependency Parsing\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "Haoshen Hong <haoshen@stanford.edu>\n",
    "\"\"\"\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParserModel(nn.Module):\n",
    "    \"\"\" Feedforward neural network with an embedding layer and two hidden layers.\n",
    "    The ParserModel will predict which transition should be applied to a\n",
    "    given partial parse configuration.\n",
    "\n",
    "    PyTorch Notes:\n",
    "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
    "            are a subclass of this \"nn.Module\".\n",
    "        - The \"__init__\" method is where you define all the layers and parameters\n",
    "            (embedding layers, linear layers, dropout layers, etc.).\n",
    "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
    "            when you write \"m = ParserModel()\".\n",
    "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
    "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
    "            in other ParserModel methods.\n",
    "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, n_features=36,\n",
    "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
    "        \"\"\" Initialize the parser model.\n",
    "\n",
    "        @param embeddings (ndarray): word embeddings (num_words, embedding_size)\n",
    "        @param n_features (int): number of input features\n",
    "        @param hidden_size (int): number of hidden units\n",
    "        @param n_classes (int): number of output classes\n",
    "        @param dropout_prob (float): dropout probability\n",
    "        \"\"\"\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embeddings = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        ### YOUR CODE HERE (~10 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) Declare `self.embed_to_hidden_weight` and `self.embed_to_hidden_bias` as `nn.Parameter`.\n",
    "        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n",
    "        ###        with default parameters.\n",
    "        ###     2) Construct `self.dropout` layer.\n",
    "        ###     3) Declare `self.hidden_to_logits_weight` and `self.hidden_to_logits_bias` as `nn.Parameter`.\n",
    "        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n",
    "        ###        with default parameters.\n",
    "        ###\n",
    "        ### Note: Trainable variables are declared as `nn.Parameter` which is a commonly used API\n",
    "        ###       to include a tensor into a computational graph to support updating w.r.t its gradient.\n",
    "        ###       Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
    "        ###       It has been shown empirically, that this provides better initial weights\n",
    "        ###       for training networks than random uniform initialization.\n",
    "        ###       For more details checkout this great blogpost:\n",
    "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     nn.Parameter: https://pytorch.org/docs/stable/nn.html#parameters\n",
    "        ###     Initialization: https://pytorch.org/docs/stable/nn.init.html\n",
    "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers\n",
    "        self.embed_to_hidden_weight=torch.nn.Parameter(torch.zeros(self.embed_size,self.hidden_size))\n",
    "        nn.init.xavier_uniform_(embed_to_hidden_weight)\n",
    "        \n",
    "        self.embed_to_hidden_bias=torch.nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        nn.init.uniform_(embed_to_hidden_bias)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=self.dropout_prob)\n",
    "        \n",
    "        self.hidden_to_logits_weight=torch.nn.Parameter(torch.zeros(self.hidden_size,self.n_classes))\n",
    "        nn.init.xavier_uniform_(self.hidden_to_logits_weight)\n",
    "        \n",
    "        self.hidden_to_logits_bias=torch.nn.Parameter(torch.zeros(self.n_classes))\n",
    "        nn.init.uniform_(self.hidden_to_logits_bias)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def embedding_lookup(self, w):\n",
    "        \"\"\" Utilize `w` to select embeddings from embedding matrix `self.embeddings`\n",
    "            @param w (Tensor): input tensor of word indices (batch_size, n_features)\n",
    "\n",
    "            @return x (Tensor): tensor of embeddings for words represented in w\n",
    "                                (batch_size, n_features * embed_size)\n",
    "        \"\"\"\n",
    "\n",
    "        ### YOUR CODE HERE (~1-3 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) For each index `i` in `w`, select `i`th vector from self.embeddings\n",
    "        ###     2) Reshape the tensor using `view` function if necessary\n",
    "        ###\n",
    "        ### Note: All embedding vectors are stacked and stored as a matrix. The model receives\n",
    "        ###       a list of indices representing a sequence of words, then it calls this lookup\n",
    "        ###       function to map indices to sequence of embeddings.\n",
    "        ###\n",
    "        ###       This problem aims to test your understanding of embedding lookup,\n",
    "        ###       so DO NOT use any high level API like nn.Embedding\n",
    "        ###       (we are asking you to implement that!). Pay attention to tensor shapes\n",
    "        ###       and reshape if necessary. Make sure you know each tensor's shape before you run the code!\n",
    "        ###\n",
    "        ### Pytorch has some useful APIs for you, and you can use either one\n",
    "        ### in this problem (except nn.Embedding). These docs might be helpful:\n",
    "        ###     Index select: https://pytorch.org/docs/stable/torch.html#torch.index_select\n",
    "        ###     Gather: https://pytorch.org/docs/stable/torch.html#torch.gather\n",
    "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "        #self.embedding=word embeddings (num_words, embedding_size)\n",
    "        #x = self.embeddings(w)\n",
    "        #x = x.view(-1, self.n_features * self.embed_size) \n",
    "        x=torch.zeros(shape=(len(w),self.n_features,self.embed_Size),requires_grad=True)\n",
    "        for i in range(len(w)):\n",
    "            #x[i]=\n",
    "            for j in range(len(self.n_features)):\n",
    "                x[i][j]=self.embeddings[w[i][j]]\n",
    "        x.reshape(len(w),self.n_features*self.embed_Size)\n",
    "            \n",
    "         \n",
    "            \n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, w):\n",
    "        \"\"\" Run the model forward.\n",
    "\n",
    "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
    "                - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor.\n",
    "                    For example, if you created an instance of your ParserModel and applied it to some `w` as follows,\n",
    "                    the `forward` function would called on `w` and the result would be stored in the `output` variable:\n",
    "                        model = ParserModel()\n",
    "                        output = model(w) # this calls the forward function\n",
    "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "\n",
    "        @param w (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
    "                                 without applying softmax (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~3-5 lines)\n",
    "        ### TODO:\n",
    "        ###     Complete the forward computation as described in write-up. In addition, include a dropout layer\n",
    "        ###     as decleared in `__init__` after ReLU function.\n",
    "        ###\n",
    "        ### Note: We do not apply the softmax to the logits here, because\n",
    "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     Matrix product: https://pytorch.org/docs/stable/torch.html#torch.matmul\n",
    "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
    "        x=self.embedding_lookup(w)\n",
    "        h=(x*self.embed_to_hidden_weight+self.embed_to_hidden_bias)\n",
    "        h=F.relu(h)\n",
    "        l=h*self.hidden_to_logits_weight+self.hidden_to_logits_bias\n",
    "        logits=l\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-e]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/subhodip/.local/share/jupyter/runtime/kernel-d079321f-1eb2-4c22-9a0b-1c86080faaa3.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Simple sanity check for parser_model.py')\n",
    "parser.add_argument('-e', '--embedding', action='store_true', help='sanity check for embeding_lookup function')\n",
    "#parser.add_argument('-f', '--forward', action='store_true', help='sanity check for forward function')\n",
    "args = parser.parse_args()\n",
    "\n",
    "embeddings = np.zeros((100, 30), dtype=np.float32)\n",
    "model = ParserModel(embeddings)\n",
    "\n",
    "def check_embedding():\n",
    "    inds = torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
    "    selected = model.embedding_lookup(inds)\n",
    "    assert np.all(selected.data.numpy() == 0), \"The result of embedding lookup: \" \\\n",
    "                                  + repr(selected) + \" contains non-zero elements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-e] [-f]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: /home/subhodip/.local/share/jupyter/runtime/kernel-d079321f-1eb2-4c22-9a0b-1c86080faaa3.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subhodip/Studies_software/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Simple sanity check for parser_model.py')\n",
    "    parser.add_argument('-e', '--embedding', action='store_true', help='sanity check for embeding_lookup function')\n",
    "    parser.add_argument('-f', '--forward', action='store_true', help='sanity check for forward function')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    embeddings = np.zeros((100, 30), dtype=np.float32)\n",
    "    model = ParserModel(embeddings)\n",
    "\n",
    "    def check_embedding():\n",
    "        inds = torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
    "        selected = model.embedding_lookup(inds)\n",
    "        assert np.all(selected.data.numpy() == 0), \"The result of embedding lookup: \" \\\n",
    "                                      + repr(selected) + \" contains non-zero elements.\"\n",
    "\n",
    "    def check_forward():\n",
    "        inputs =torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
    "        out = model(inputs)\n",
    "        expected_out_shape = (4, 3)\n",
    "        assert out.shape == expected_out_shape, \"The result shape of forward is: \" + repr(out.shape) + \\\n",
    "                                                \" which doesn't match expected \" + repr(expected_out_shape)\n",
    "\n",
    "    if args.embedding:\n",
    "        check_embedding()\n",
    "        print(\"Embedding_lookup sanity check passes!\")\n",
    "\n",
    "    if args.forward:\n",
    "        check_forward()\n",
    "        print(\"Forward sanity check passes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3820, -0.0677,  0.5119,  0.2736, -0.7163,  0.7563],\n",
      "        [-0.4675, -0.1687, -0.3103,  0.2601, -0.5846, -0.7143],\n",
      "        [-0.5724,  0.1849, -0.3043, -0.7420, -0.7944, -0.7968]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Parameter' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-81a9b3b1d4dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_to_hidden_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_to_hidden_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_to_hidden_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Parameter' object is not callable"
     ]
    }
   ],
   "source": [
    "embed_size=3\n",
    "hidden_size=6\n",
    "embed_to_hidden_weight=torch.nn.Parameter(torch.zeros(embed_size,hidden_size))\n",
    "nn.init.xavier_uniform_(embed_to_hidden_weight)\n",
    "print(embed_to_hidden_weight)\n",
    "c=embed_to_hidden_weight(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5464, 0.5943, 0.7009],\n",
      "        [0.0338, 0.9006, 0.4414],\n",
      "        [0.8366, 0.6940, 0.1680],\n",
      "        [0.2177, 0.4526, 0.7647],\n",
      "        [0.5192, 0.6035, 0.9593]])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.7375, 0.7039, 0.8040], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "embed_size=3\n",
    "hidden_size=6\n",
    "embed_to_hidden_weight=torch.nn.Parameter(torch.zeros(embed_size))\n",
    "nn.init.uniform_(embed_to_hidden_weight)\n",
    "print(embed_to_hidden_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(10, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4534, -0.2933, -0.9490],\n",
       "         [ 0.0184,  0.6152, -0.3442],\n",
       "         [-1.4534, -0.2933, -0.9490],\n",
       "         [ 1.1815,  1.2855,  1.8208]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[0,2,0,5]])\n",
    "print(embedding)\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
