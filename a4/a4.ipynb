{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"a4.ipynb","provenance":[],"authorship_tag":"ABX9TyMrpb8KbrGkD5WtaJtmLq0i"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"SSJIHucJ3HUr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":911},"executionInfo":{"status":"error","timestamp":1594938296942,"user_tz":300,"elapsed":2029,"user":{"displayName":"Subhodip Saha","photoUrl":"https://lh4.googleusercontent.com/-A4vfF5DeLH0/AAAAAAAAAAI/AAAAAAAAAh8/UijQ_Akhn3k/s64/photo.jpg","userId":"01295837312679332258"}},"outputId":"c28730db-7078-4e3e-a93c-957af31ba32f"},"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = 'AI/ManningNLP/a4'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# this downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","#%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n","#!bash get_datasets.sh\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/\n","%cd /content"],"execution_count":182,"outputs":[{"output_type":"stream","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Mounted at /content/drive\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-182-1c0285daf071>\", line 20, in <module>\n","    get_ipython().magic('cd /content/drive/My\\\\ Drive/$FOLDERNAME/')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-91>\", line 2, in cd\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/osm.py\", line 288, in cd\n","    oldcwd = py3compat.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.6/posixpath.py\", line 383, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"7NzuP2kzuGz3","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzvGHvprRVx_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594930591664,"user_tz":300,"elapsed":607,"user":{"displayName":"Subhodip Saha","photoUrl":"https://lh4.googleusercontent.com/-A4vfF5DeLH0/AAAAAAAAAAI/AAAAAAAAAh8/UijQ_Akhn3k/s64/photo.jpg","userId":"01295837312679332258"}},"outputId":"ed132a88-07b7-469d-cd24-f9757db5868c"},"source":["#USE_GPU = True\n","USE_GPU=False\n","\n","dtype = torch.float32 # we will be using float throughout this tutorial\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","# Constant to control how frequently we print train loss\n","print_every = 100\n","\n","print('using device:', device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using device: cpu\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tEUC_MvwKcRH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1594928785550,"user_tz":300,"elapsed":2244,"user":{"displayName":"Subhodip Saha","photoUrl":"https://lh4.googleusercontent.com/-A4vfF5DeLH0/AAAAAAAAAAI/AAAAAAAAAh8/UijQ_Akhn3k/s64/photo.jpg","userId":"01295837312679332258"}},"outputId":"8e451ed1-7365-41e7-f6f7-6ad55549d6fb"},"source":["%ls -la"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 20\n","drwxr-xr-x 1 root root 4096 Jul 16 19:46 \u001b[0m\u001b[01;34m.\u001b[0m/\n","drwxr-xr-x 1 root root 4096 Jul 16 15:51 \u001b[01;34m..\u001b[0m/\n","drwxr-xr-x 1 root root 4096 Jul 13 16:14 \u001b[01;34m.config\u001b[0m/\n","drwx------ 4 root root 4096 Jul 16 19:46 \u001b[01;34mdrive\u001b[0m/\n","drwxr-xr-x 1 root root 4096 Jul 10 16:29 \u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RmUHJf2DVAT9","colab_type":"code","colab":{}},"source":["import math\n","from typing import List\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import nltk\n","#nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxoJk4fkVMQO","colab_type":"code","colab":{}},"source":["def pad_sents(sents, pad_token):\n","    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n","        The paddings should be at the end of each sentence.\n","    @param sents (list[list[str]]): list of sentences, where each sentence\n","                                    is represented as a list of words\n","    @param pad_token (str): padding token\n","    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n","        than the max length sentence are padded out with the pad_token, such that\n","        each sentences in the batch now has equal length.\n","    \"\"\"\n","    sents_padded = []\n","\n","    ### YOUR CODE HERE (~6 Lines)\n","\n","    #print('sents',len(sents))\n","    maxlen=max(len(sent) for sent in sents)\n","    for i in range(len(sents)):\n","      toadd=maxlen-len(sents[i])\n","      for j in range(toadd):\n","        sents[i].append(pad_token)\n","\n","    sents_padded=sents  \n","\n","\n","\n","    ### END YOUR CODE\n","\n","    return sents_padded"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTpeIbzIYO_n","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3wdq0VZZX_8L","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","\n","class ModelEmbeddings(nn.Module): \n","    \"\"\"\n","    Class that converts input words to their embeddings.\n","    \"\"\"\n","    def __init__(self, embed_size, vocab):\n","        \"\"\"\n","        Init the Embedding layers.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        \"\"\"\n","        super(ModelEmbeddings, self).__init__()\n","        self.embed_size = embed_size\n","\n","        # default values\n","        self.source = None\n","        self.target = None\n","\n","        src_pad_token_idx = vocab.src['<pad>']\n","        tgt_pad_token_idx = vocab.tgt['<pad>']\n","\n","        ### YOUR CODE HERE (~2 Lines)\n","        ### TODO - Initialize the following variables:\n","        ###     self.source (Embedding Layer for source language)\n","        ###     self.target (Embedding Layer for target langauge)\n","        ###\n","        ### Note:\n","        ###     1. `vocab` object contains two vocabularies:\n","        ###            `vocab.src` for source\n","        ###            `vocab.tgt` for target\n","        ###     2. You can get the length of a specific vocabulary by running:\n","        ###             `len(vocab.<specific_vocabulary>)`\n","        ###     3. Remember to include the padding token for the specific vocabulary\n","        ###        when creating your Embedding.\n","        ###\n","        ### Use the following docs to properly initialize these variables:\n","        ###     Embedding Layer:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n","        #print(type(src_pad_token_idx))\n","        #print(tgt_pad_token_idx)\n","        self.source = nn.Embedding(vocab.src.__len__(), embed_size)\n","        #self.source.weight = nn.Parameter(torch.tensor(vocab.src.__len__()))\n","\n","        self.target = nn.Embedding(vocab.tgt.__len__(), embed_size)\n","        #self.target.weight = nn.Parameter(torch.tensor(tgt_pad_token_idx))\n","\n","\n","\n","        \n","\n","        ### END YOUR CODE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LH_G2BbphOSE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":493},"executionInfo":{"status":"error","timestamp":1594937131261,"user_tz":300,"elapsed":717,"user":{"displayName":"Subhodip Saha","photoUrl":"https://lh4.googleusercontent.com/-A4vfF5DeLH0/AAAAAAAAAAI/AAAAAAAAAh8/UijQ_Akhn3k/s64/photo.jpg","userId":"01295837312679332258"}},"outputId":"87433f6c-7808-49a0-8aa1-b85b61850912"},"source":["main('1d')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vocab 85\n","model cpu\n","device cpu\n","Running Sanity Check for Question 1d: Encode\n","--------------------------------------------------------------------------------\n","source_padded_chk <class 'torch.Tensor'>\n","source_padded torch.Size([0])\n","source_lengths [22, 14, 10, 10, 6]\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-174-5e5d9106ce7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-170-0618ddabc464>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     46\u001b[0m         '''\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'1d'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mquestion_1d_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1e'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mquestion_1e_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-171-56255eb17b3a>\u001b[0m in \u001b[0;36mquestion_1d_sanity_check\u001b[0;34m(model, src_sents, tgt_sents, vocab)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0menc_hiddens_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_init_state_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_hiddens_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hiddens_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"enc_hiddens is incorrect: it should be:\\n {} but is:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_hiddens_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hiddens_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"enc_hiddens Sanity Checks Passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_init_state_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_init_state_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dec_init_state[0] is incorrect: it should be:\\n {} but is:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_init_state_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_init_state_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"]}]},{"cell_type":"code","metadata":{"id":"x73V4wrDYSVg","colab_type":"code","colab":{}},"source":["from collections import namedtuple\n","import sys\n","from typing import List, Tuple, Dict, Set, Union\n","import torch\n","import torch.nn as nn\n","import torch.nn.utils\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","\n","#from model_embeddings import ModelEmbeddings\n","Hypothesis = namedtuple('Hypothesis', ['value', 'score'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQO_iXOJhO_a","colab_type":"code","colab":{}},"source":["\n","\n","\n","class NMT(nn.Module):\n","    \"\"\" Simple Neural Machine Translation Model:\n","        - Bidrectional LSTM Encoder\n","        - Unidirection LSTM Decoder\n","        - Global Attention Model (Luong, et al. 2015)\n","    \"\"\"\n","    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n","        \"\"\" Init NMT Model.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param hidden_size (int): Hidden Size, the size of hidden states (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        @param dropout_rate (float): Dropout probability, for attention\n","        \"\"\"\n","        super(NMT, self).__init__()\n","        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n","        self.hidden_size = hidden_size\n","        self.dropout_rate = dropout_rate\n","        self.vocab = vocab\n","\n","        # default values\n","        self.encoder = None \n","        self.decoder = None\n","        self.h_projection = None\n","        self.c_projection = None\n","        self.att_projection = None\n","        self.combined_output_projection = None\n","        self.target_vocab_projection = None\n","        self.dropout = None\n","        # For sanity check only, not relevant to implementation\n","        self.gen_sanity_check = False\n","        self.counter = 0\n","\n","\n","        #self.device=device #I inserted this    \n","        print('model',self.model_embeddings.source.weight.device)\n","        #print()\n","\n","        ### YOUR CODE HERE (~8 Lines)\n","        ### TODO - Initialize the following variables:\n","        ###     self.encoder (Bidirectional LSTM with bias)\n","        ###     self.decoder (LSTM Cell with bias)\n","        ###     self.h_projection (Linear Layer with no bias), called W_{h} in the PDF.\n","        ###     self.c_projection (Linear Layer with no bias), called W_{c} in the PDF.\n","        ###     self.att_projection (Linear Layer with no bias), called W_{attProj} in the PDF.\n","        ###     self.combined_output_projection (Linear Layer with no bias), called W_{u} in the PDF.\n","        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab} in the PDF.\n","        ###     self.dropout (Dropout Layer)\n","        ###\n","        ### Use the following docs to properly initialize these variables:\n","        ###     LSTM:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n","        ###     LSTM Cell:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell\n","        ###     Linear Layer:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n","        ###     Dropout Layer:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n","        self.encoder = nn.LSTM(embed_size, hidden_size, num_layers=1,  bidirectional=True)\n","        self.decoder=nn.LSTMCell(embed_size,hidden_size)\n","\n","        self.h_projection=nn.Linear(2*hidden_size,hidden_size,bias=False)\n","        self.c_projection=nn.Linear(2*hidden_size,hidden_size,bias=False)\n","\n","        self.att_projection=nn.Linear(2*hidden_size,hidden_size,bias=False)\n","        self.combined_output_projection=nn.Linear(3*hidden_size,hidden_size,bias=False)\n","\n","        self.target_vocab_projection=nn.Linear(hidden_size,vocab.tgt.__len__(),bias=False)\n","        self.dropout=nn.Dropout(p=dropout_rate)\n","\n","\n","\n","\n","\n","        ### END YOUR CODE\n","\n","\n","    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n","        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n","        target sentences under the language models learned by the NMT system.\n","\n","        @param source (List[List[str]]): list of source sentence tokens\n","        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n","\n","        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n","                                    log-likelihood of generating the gold-standard target sentence for\n","                                    each example in the input batch. Here b = batch size.\n","        \"\"\"\n","        # Compute sentence lengths\n","        source_lengths = [len(s) for s in source]\n","\n","        # Convert list of lists into tensors\n","        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n","        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n","\n","        ###     Run the network forward:\n","        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`\n","        ###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`\n","        ###     3. Apply the decoder to compute combined-output by calling `self.decode()`\n","        ###     4. Compute log probability distribution over the target vocabulary using the\n","        ###        combined_outputs returned by the `self.decode()` function.\n","\n","        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n","        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n","        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n","        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n","\n","        # Zero out, probabilities for which we have nothing in the target text\n","        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n","        \n","        # Compute log probability of generating true target words\n","        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n","        scores = target_gold_words_log_prob.sum(dim=0)\n","        return scores\n","\n","\n","    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n","            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n","\n","        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n","                                        b = batch_size, src_len = maximum source sentence length. Note that \n","                                       these have already been sorted in order of longest to shortest sentence.\n","        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n","        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n","                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n","                                                hidden state and cell.\n","        \"\"\"\n","        enc_hiddens, dec_init_state = None, None\n","\n","        ### YOUR CODE HERE (~ 8 Lines)\n","        ### TODO:\n","        ###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.\n","        ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note\n","        ###         that there is no initial hidden state or cell for the decoder.\n","        ###     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.\n","        ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.\n","        ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.\n","        ###         - Note that the shape of the tensor returned by the encoder is (src_len, b, h*2) and we want to\n","        ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.\n","        ###     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):\n","        ###         - `init_decoder_hidden`:\n","        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n","        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n","        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.\n","        ###             This is h_0^{dec} in the PDF. Here b = batch size, h = hidden size\n","        ###         - `init_decoder_cell`:\n","        ###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n","        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n","        ###             Apply the c_projection layer to this in order to compute init_decoder_cell.\n","        ###             This is c_0^{dec} in the PDF. Here b = batch size, h = hidden size\n","        ###\n","        ### See the following docs, as you may need to use some of the following functions in your implementation:\n","        ###     Pack the padded sequence X before passing to the encoder:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n","        ###     Pad the packed sequence, enc_hiddens, returned by the encoder:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence\n","        ###     Tensor Concatenation:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n","        ###     Tensor Permute:\n","        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute\n","        print('source_padded',source_padded.shape)\n","        print('source_lengths',source_lengths)\n","        #source_padded: torch.Tensor, source_lengths: List[int]\n","        #X=\n","\n","   \n","\n","\n","        ### END YOUR CODE\n","\n","        return enc_hiddens, dec_init_state\n","\n","\n","    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n","                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Compute combined output vectors for a batch.\n","\n","        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n","                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n","                                     b = batch size, src_len = maximum source sentence length.\n","        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n","        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n","                                       tgt_len = maximum target sentence length, b = batch size. \n","\n","        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n","                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n","        \"\"\"\n","        # Chop of the <END> token for max length sentences.\n","        target_padded = target_padded[:-1]\n","\n","        # Initialize the decoder state (hidden and cell)\n","        dec_state = dec_init_state\n","\n","        # Initialize previous combined output vector o_{t-1} as zero\n","        batch_size = enc_hiddens.size(0)\n","        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n","\n","        # Initialize a list we will use to collect the combined output o_t on each step\n","        combined_outputs = []\n","\n","        ### YOUR CODE HERE (~9 Lines)\n","        ### TODO:\n","        ###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,\n","        ###         which should be shape (b, src_len, h),\n","        ###         where b = batch size, src_len = maximum source length, h = hidden size.\n","        ###         This is applying W_{attProj} to h^enc, as described in the PDF.\n","        ###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.\n","        ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n","        ###     3. Use the torch.split function to iterate over the time dimension of Y.\n","        ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.\n","        ###             - Squeeze Y_t into a tensor of dimension (b, e). \n","        ###             - Construct Ybar_t by concatenating Y_t with o_prev on their last dimension\n","        ###             - Use the step function to compute the the Decoder's next (cell, state) values\n","        ###               as well as the new combined output o_t.\n","        ###             - Append o_t to combined_outputs\n","        ###             - Update o_prev to the new o_t.\n","        ###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of\n","        ###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)\n","        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.\n","        ###\n","        ### Note:\n","        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze\n","        ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n","        ###   \n","        ### You may find some of these functions useful:\n","        ###     Zeros Tensor:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.zeros\n","        ###     Tensor Splitting (iteration):\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.split\n","        ###     Tensor Dimension Squeezing:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n","        ###     Tensor Concatenation:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n","        ###     Tensor Stacking:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.stack\n","\n","\n","\n","\n","\n","\n","        ### END YOUR CODE\n","\n","        return combined_outputs\n","\n","\n","    def step(self, Ybar_t: torch.Tensor,\n","            dec_state: Tuple[torch.Tensor, torch.Tensor],\n","            enc_hiddens: torch.Tensor,\n","            enc_hiddens_proj: torch.Tensor,\n","            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n","        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n","\n","        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n","                                where b = batch size, e = embedding size, h = hidden size.\n","        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n","                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n","        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n","                                    src_len = maximum source length, h = hidden size.\n","        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n","                                    where b = batch size, src_len = maximum source length, h = hidden size.\n","        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n","                                    where b = batch size, src_len is maximum source length. \n","\n","        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n","                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n","        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n","        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n","                                Note: You will not use this outside of this function.\n","                                      We are simply returning this value so that we can sanity check\n","                                      your implementation.\n","        \"\"\"\n","\n","        combined_output = None\n","\n","        ### YOUR CODE HERE (~3 Lines)\n","        ### TODO:\n","        ###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.\n","        ###     2. Split dec_state into its two parts (dec_hidden, dec_cell)\n","        ###     3. Compute the attention scores e_t, a Tensor shape (b, src_len). \n","        ###        Note: b = batch_size, src_len = maximum source length, h = hidden size.\n","        ###\n","        ###       Hints:\n","        ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)\n","        ###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_{attProj} h^enc (batched).\n","        ###         - Use batched matrix multiplication (torch.bmm) to compute e_t (be careful about the input/ output shapes!)\n","        ###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.\n","        ###         - When using the squeeze() function make sure to specify the dimension you want to squeeze\n","        ###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n","        ###\n","        ### Use the following docs to implement this functionality:\n","        ###     Batch Multiplication:\n","        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n","        ###     Tensor Unsqueeze:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n","        ###     Tensor Squeeze:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n","\n","\n","        ### END YOUR CODE\n","\n","        # Set e_t to -inf where enc_masks has 1\n","        if enc_masks is not None:\n","            e_t.data.masked_fill_(enc_masks.bool(), -float('inf'))\n","\n","        ### YOUR CODE HERE (~6 Lines)\n","        ### TODO:\n","        ###     1. Apply softmax to e_t to yield alpha_t\n","        ###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the\n","        ###         attention output vector, a_t.\n","        ###           - alpha_t is shape (b, src_len)\n","        ###           - enc_hiddens is shape (b, src_len, 2h)\n","        ###           - a_t should be shape (b, 2h)\n","        ###           - You will need to do some squeezing and unsqueezing.\n","        ###     Note: b = batch size, src_len = maximum source length, h = hidden size.\n","        ###\n","        ###     3. Concatenate dec_hidden with a_t to compute tensor U_t\n","        ###     4. Apply the combined output projection layer to U_t to compute tensor V_t\n","        ###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.\n","        ###\n","        ### Use the following docs to implement this functionality:\n","        ###     Softmax:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax\n","        ###     Batch Multiplication:\n","        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n","        ###     Tensor View:\n","        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n","        ###     Tensor Concatenation:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n","        ###     Tanh:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.tanh\n","\n","\n","        ### END YOUR CODE\n","\n","        combined_output = O_t\n","        return dec_state, combined_output, e_t\n","\n","    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n","        \"\"\" Generate sentence masks for encoder hidden states.\n","\n","        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n","                                     src_len = max source length, h = hidden size. \n","        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n","        \n","        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n","                                    where src_len = max source length, h = hidden size.\n","        \"\"\"\n","        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n","        for e_id, src_len in enumerate(source_lengths):\n","            enc_masks[e_id, src_len:] = 1\n","        return enc_masks.to(self.device)\n","\n","\n","    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n","        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n","        @param src_sent (List[str]): a single source sentence (words)\n","        @param beam_size (int): beam size\n","        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n","        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n","                value: List[str]: the decoded target sentence, represented as a list of words\n","                score: float: the log-likelihood of the target sentence\n","        \"\"\"\n","        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n","\n","        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n","        src_encodings_att_linear = self.att_projection(src_encodings)\n","\n","        h_tm1 = dec_init_vec\n","        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n","\n","        eos_id = self.vocab.tgt['</s>']\n","\n","        hypotheses = [['<s>']]\n","        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n","        completed_hypotheses = []\n","\n","        t = 0\n","        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n","            t += 1\n","            hyp_num = len(hypotheses)\n","\n","            exp_src_encodings = src_encodings.expand(hyp_num,\n","                                                     src_encodings.size(1),\n","                                                     src_encodings.size(2))\n","\n","            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n","                                                                           src_encodings_att_linear.size(1),\n","                                                                           src_encodings_att_linear.size(2))\n","\n","            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n","            y_t_embed = self.model_embeddings.target(y_tm1)\n","\n","            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n","\n","            (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n","                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n","\n","            # log probabilities over target words\n","            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n","\n","            live_hyp_num = beam_size - len(completed_hypotheses)\n","            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n","            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n","\n","            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n","            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n","\n","            new_hypotheses = []\n","            live_hyp_ids = []\n","            new_hyp_scores = []\n","\n","            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n","                prev_hyp_id = prev_hyp_id.item()\n","                hyp_word_id = hyp_word_id.item()\n","                cand_new_hyp_score = cand_new_hyp_score.item()\n","\n","                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n","                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n","                if hyp_word == '</s>':\n","                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n","                                                           score=cand_new_hyp_score))\n","                else:\n","                    new_hypotheses.append(new_hyp_sent)\n","                    live_hyp_ids.append(prev_hyp_id)\n","                    new_hyp_scores.append(cand_new_hyp_score)\n","\n","            if len(completed_hypotheses) == beam_size:\n","                break\n","\n","            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n","            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n","            att_tm1 = att_t[live_hyp_ids]\n","\n","            hypotheses = new_hypotheses\n","            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n","\n","        if len(completed_hypotheses) == 0:\n","            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n","                                                   score=hyp_scores[0].item()))\n","\n","        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n","\n","        return completed_hypotheses\n","\n","    @property\n","    def device(self) -> torch.device:\n","        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","        \"\"\"\n","        #print('inside ')\n","        return self.model_embeddings.source.weight.device\n","\n","    @staticmethod\n","    def load(model_path: str):\n","        \"\"\" Load the model from a file.\n","        @param model_path (str): path to model\n","        \"\"\"\n","        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","        args = params['args']\n","        model = NMT(vocab=params['vocab'], **args)\n","        model.load_state_dict(params['state_dict'])\n","\n","        return model\n","\n","    def save(self, path: str):\n","        \"\"\" Save the odel to a file.\n","        @param path (str): path to the model\n","        \"\"\"\n","        print('save model parameters to [%s]' % path, file=sys.stderr)\n","\n","        params = {\n","            'args': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n","            'vocab': self.vocab,\n","            'state_dict': self.state_dict()\n","        }\n","\n","        torch.save(params, path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7eq-KyRYXVZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"error","timestamp":1594937066951,"user_tz":300,"elapsed":663,"user":{"displayName":"Subhodip Saha","photoUrl":"https://lh4.googleusercontent.com/-A4vfF5DeLH0/AAAAAAAAAAI/AAAAAAAAAh8/UijQ_Akhn3k/s64/photo.jpg","userId":"01295837312679332258"}},"outputId":"297deda1-3ddb-4dfb-9316-4845c40b81ff"},"source":["from utils import read_corpus, pad_sents\n","main('1d')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vocab 85\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-169-30122c9fb218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_sents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-109-0618ddabc464>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         vocab=vocab)\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     '''\n","\u001b[0;32m<ipython-input-136-f1997d46b0b3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_size, hidden_size, vocab, dropout_rate)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#self.device=device #I inserted this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m#print()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'weight'"]}]},{"cell_type":"code","metadata":{"id":"WVWMj_0JWmyH","colab_type":"code","colab":{}},"source":["def main(args):\n","    \"\"\" Main func.\n","    \"\"\"\n","    #args = docopt(__doc__)\n","\n","    # Check Python & PyTorch Versions\n","    assert (sys.version_info >= (3, 5)), \"Please update your installation of Python to version >= 3.5\"\n","    assert(torch.__version__ >= \"1.0.0\"), \"Please update your installation of PyTorch. You have {} and you should have version 1.0.0\".format(torch.__version__)\n","\n","    # Seed the Random Number Generators\n","    seed = 1234\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    np.random.seed(seed * 13 // 7)\n","\n","    # Load training data & vocabulary\n","    train_data_src = read_corpus('./sanity_check_en_es_data/train_sanity_check.es', 'src')\n","    train_data_tgt = read_corpus('./sanity_check_en_es_data/train_sanity_check.en', 'tgt')\n","    train_data = list(zip(train_data_src, train_data_tgt))\n","\n","    for src_sents, tgt_sents in batch_iter(train_data, batch_size=BATCH_SIZE, shuffle=True):\n","        src_sents = src_sents\n","        tgt_sents = tgt_sents\n","        break\n","    vocab = Vocab.load('./sanity_check_en_es_data/vocab_sanity_check.json') \n","    print('vocab',vocab.tgt.__len__())\n","\n","    # Create NMT Model\n","    model = NMT(\n","        embed_size=EMBED_SIZE,\n","        hidden_size=HIDDEN_SIZE,\n","        dropout_rate=DROPOUT_RATE,\n","        vocab=vocab)\n","    print('device',model.device)\n","    '''\n","    if args['1d']:\n","        question_1d_sanity_check(model, src_sents, tgt_sents, vocab)\n","    elif args['1e']:\n","        question_1e_sanity_check(model, src_sents, tgt_sents, vocab)\n","    elif args['1f']:\n","        question_1f_sanity_check(model, src_sents, tgt_sents, vocab)\n","    elif args['overwrite_output_for_sanity_check']:\n","        generate_outputs(model, src_sents, tgt_sents, vocab)\n","    else:\n","        raise RuntimeError('invalid run mode')\n","        '''\n","    if args=='1d':\n","        question_1d_sanity_check(model, src_sents, tgt_sents, vocab)\n","    elif args['1e']:\n","        question_1e_sanity_check(model, src_sents, tgt_sents, vocab)\n","    elif args['1f']:\n","        question_1f_sanity_check(model, src_sents, tgt_sents, vocab)\n","    elif args['overwrite_output_for_sanity_check']:\n","        generate_outputs(model, src_sents, tgt_sents, vocab)\n","    else:\n","        raise RuntimeError('invalid run mode')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5h_5i9BLiqvR","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BWEJ5GA2u74","colab_type":"code","colab":{}},"source":["\n","\n","def reinitialize_layers(model):\n","    \"\"\" Reinitialize the Layer Weights for Sanity Checks.\n","    \"\"\"\n","    def init_weights(m):\n","        if type(m) == nn.Linear:\n","            m.weight.data.fill_(0.3)\n","            if m.bias is not None:\n","                m.bias.data.fill_(0.1)\n","        elif type(m) == nn.Embedding:\n","            m.weight.data.fill_(0.15)\n","        elif type(m) == nn.Dropout:\n","            nn.Dropout(DROPOUT_RATE)\n","    with torch.no_grad():\n","        model.apply(init_weights)\n","\n","\n","def generate_outputs(model, source, target, vocab):\n","    \"\"\" Generate outputs.\n","    \"\"\"\n","    print (\"-\"*80)\n","    print(\"Generating Comparison Outputs\")\n","    reinitialize_layers(model)\n","    model.gen_sanity_check = True\n","    model.counter = 0\n","\n","    # Compute sentence lengths\n","    source_lengths = [len(s) for s in source]\n","\n","    # Convert list of lists into tensors\n","    source_padded = model.vocab.src.to_input_tensor(source, device=model.device)\n","    target_padded = model.vocab.tgt.to_input_tensor(target, device=model.device)\n","\n","    # Run the model forward\n","    with torch.no_grad():\n","        enc_hiddens, dec_init_state = model.encode(source_padded, source_lengths)\n","        enc_masks = model.generate_sent_masks(enc_hiddens, source_lengths)\n","        combined_outputs = model.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n","\n","    # Save Tensors to disk\n","    torch.save(enc_hiddens, './sanity_check_en_es_data/enc_hiddens.pkl')\n","    torch.save(dec_init_state, './sanity_check_en_es_data/dec_init_state.pkl') \n","    torch.save(enc_masks, './sanity_check_en_es_data/enc_masks.pkl')\n","    torch.save(combined_outputs, './sanity_check_en_es_data/combined_outputs.pkl')\n","    torch.save(target_padded, './sanity_check_en_es_data/target_padded.pkl')\n","\n","    # 1f\n","    # Inputs\n","    Ybar_t = torch.load('./sanity_check_en_es_data/Ybar_t.pkl')\n","    enc_hiddens_proj = torch.load('./sanity_check_en_es_data/enc_hiddens_proj.pkl')\n","    reinitialize_layers(model)\n","    # Run Tests\n","    with torch.no_grad():\n","        dec_state_target, o_t_target, e_t_target = model.step(Ybar_t, dec_init_state, enc_hiddens, enc_hiddens_proj,\n","                                                        enc_masks)\n","    torch.save(dec_state_target, './sanity_check_en_es_data/dec_state.pkl')\n","    torch.save(o_t_target, './sanity_check_en_es_data/o_t.pkl')\n","    torch.save(e_t_target, './sanity_check_en_es_data/e_t.pkl')\n","\n","    model.gen_sanity_check = False\n","\n","def question_1d_sanity_check(model, src_sents, tgt_sents, vocab):\n","    \"\"\" Sanity check for question 1d. \n","        Compares student output to that of model with dummy data.\n","    \"\"\"\n","    print(\"Running Sanity Check for Question 1d: Encode\")\n","    print (\"-\"*80)\n","\n","    # Configure for Testing\n","    reinitialize_layers(model)\n","    source_lengths = [len(s) for s in src_sents]\n","    source_padded = model.vocab.src.to_input_tensor(src_sents, device=model.device)\n","\n","    # Load Outputs\n","    enc_hiddens_target = torch.load('./sanity_check_en_es_data/enc_hiddens.pkl')\n","    dec_init_state_target = torch.load('./sanity_check_en_es_data/dec_init_state.pkl')\n","\n","    # Test\n","    print('source_padded_chk',len(source_padded))\n","    with torch.no_grad():\n","        enc_hiddens_pred, dec_init_state_pred = model.encode(source_padded, source_lengths)\n","    assert(np.allclose(enc_hiddens_target.numpy(), enc_hiddens_pred.numpy())), \"enc_hiddens is incorrect: it should be:\\n {} but is:\\n{}\".format(enc_hiddens_target, enc_hiddens_pred)\n","    print(\"enc_hiddens Sanity Checks Passed!\")\n","    assert(np.allclose(dec_init_state_target[0].numpy(), dec_init_state_pred[0].numpy())), \"dec_init_state[0] is incorrect: it should be:\\n {} but is:\\n{}\".format(dec_init_state_target[0], dec_init_state_pred[0])\n","    print(\"dec_init_state[0] Sanity Checks Passed!\")\n","    assert(np.allclose(dec_init_state_target[1].numpy(), dec_init_state_pred[1].numpy())), \"dec_init_state[1] is incorrect: it should be:\\n {} but is:\\n{}\".format(dec_init_state_target[1], dec_init_state_pred[1])\n","    print(\"dec_init_state[1] Sanity Checks Passed!\")\n","    print (\"-\"*80)\n","    print(\"All Sanity Checks Passed for Question 1d: Encode!\")\n","    print (\"-\"*80)\n","\n","\n","def question_1e_sanity_check(model, src_sents, tgt_sents, vocab):\n","    \"\"\" Sanity check for question 1e. \n","        Compares student output to that of model with dummy data.\n","    \"\"\"\n","    print (\"-\"*80)\n","    print(\"Running Sanity Check for Question 1e: Decode\")\n","    print (\"-\"*80)\n","\n","    # Load Inputs\n","    dec_init_state = torch.load('./sanity_check_en_es_data/dec_init_state.pkl')\n","    enc_hiddens = torch.load('./sanity_check_en_es_data/enc_hiddens.pkl')\n","    enc_masks = torch.load('./sanity_check_en_es_data/enc_masks.pkl')\n","    target_padded = torch.load('./sanity_check_en_es_data/target_padded.pkl')\n","\n","    # Load Outputs\n","    combined_outputs_target = torch.load('./sanity_check_en_es_data/combined_outputs.pkl')\n","    print(combined_outputs_target.shape)\n","\n","    # Configure for Testing\n","    reinitialize_layers(model)\n","    COUNTER = [0]\n","    def stepFunction(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks):\n","       dec_state = torch.load('./sanity_check_en_es_data/step_dec_state_{}.pkl'.format(COUNTER[0]))\n","       o_t = torch.load('./sanity_check_en_es_data/step_o_t_{}.pkl'.format(COUNTER[0]))\n","       COUNTER[0]+=1\n","       return dec_state, o_t, None\n","    model.step = stepFunction\n","\n","    # Run Tests\n","    with torch.no_grad():\n","        combined_outputs_pred = model.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n","    assert(np.allclose(combined_outputs_pred.numpy(), combined_outputs_target.numpy())), \"combined_outputs is incorrect: it should be:\\n {} but is:\\n{}\".format(combined_outputs_target, combined_outputs_pred)\n","    print(\"combined_outputs Sanity Checks Passed!\")\n","    print (\"-\"*80)\n","    print(\"All Sanity Checks Passed for Question 1e: Decode!\")\n","    print (\"-\"*80)\n","\n","def question_1f_sanity_check(model, src_sents, tgt_sents, vocab):\n","    \"\"\" Sanity check for question 1f. \n","        Compares student output to that of model with dummy data.\n","    \"\"\"\n","    print (\"-\"*80)\n","    print(\"Running Sanity Check for Question 1f: Step\")\n","    print (\"-\"*80)\n","    reinitialize_layers(model)\n","\n","    # Inputs\n","    Ybar_t = torch.load('./sanity_check_en_es_data/Ybar_t.pkl')\n","    dec_init_state = torch.load('./sanity_check_en_es_data/dec_init_state.pkl')\n","    enc_hiddens = torch.load('./sanity_check_en_es_data/enc_hiddens.pkl')\n","    enc_masks = torch.load('./sanity_check_en_es_data/enc_masks.pkl')\n","    enc_hiddens_proj = torch.load('./sanity_check_en_es_data/enc_hiddens_proj.pkl')\n","\n","    # Output\n","    dec_state_target = torch.load('./sanity_check_en_es_data/dec_state.pkl')\n","    o_t_target = torch.load('./sanity_check_en_es_data/o_t.pkl')\n","    e_t_target = torch.load('./sanity_check_en_es_data/e_t.pkl')\n","\n","    # Run Tests\n","    with torch.no_grad():\n","        dec_state_pred, o_t_pred, e_t_pred= model.step(Ybar_t, dec_init_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n","    assert(np.allclose(dec_state_target[0].numpy(), dec_state_pred[0].numpy())), \"decoder_state[0] is incorrect: it should be:\\n {} but is:\\n{}\".format(dec_state_target[0], dec_state_pred[0])\n","    print(\"dec_state[0] Sanity Checks Passed!\")\n","    assert(np.allclose(dec_state_target[1].numpy(), dec_state_pred[1].numpy())), \"decoder_state[1] is incorrect: it should be:\\n {} but is:\\n{}\".format(dec_state_target[1], dec_state_pred[1])\n","    print(\"dec_state[1] Sanity Checks Passed!\")\n","    assert(np.allclose(o_t_target.numpy(), o_t_pred.numpy())), \"combined_output is incorrect: it should be:\\n {} but is:\\n{}\".format(o_t_target, o_t_pred)\n","    print(\"combined_output  Sanity Checks Passed!\")\n","    assert(np.allclose(e_t_target.numpy(), e_t_pred.numpy())), \"e_t is incorrect: it should be:\\n {} but is:\\n{}\".format(e_t_target, e_t_pred)\n","    print(\"e_t Sanity Checks Passed!\")\n","    print (\"-\"*80)    \n","    print(\"All Sanity Checks Passed for Question 1f: Step!\")\n","    print (\"-\"*80)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFq4cEki2wqo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":476},"executionInfo":{"status":"error","timestamp":1594937199531,"user_tz":300,"elapsed":653,"user":{"displayName":"Subhodip Saha","photoUrl":"https://lh4.googleusercontent.com/-A4vfF5DeLH0/AAAAAAAAAAI/AAAAAAAAAh8/UijQ_Akhn3k/s64/photo.jpg","userId":"01295837312679332258"}},"outputId":"6f00a376-3d0e-4737-e732-1825d0a60990"},"source":["main('1d')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vocab 85\n","model cpu\n","device cpu\n","Running Sanity Check for Question 1d: Encode\n","--------------------------------------------------------------------------------\n","source_padded_chk 0\n","source_padded torch.Size([0])\n","source_lengths [22, 14, 10, 10, 6]\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-176-5e5d9106ce7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-170-0618ddabc464>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     46\u001b[0m         '''\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'1d'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mquestion_1d_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1e'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mquestion_1e_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-175-06ac1e1b8da4>\u001b[0m in \u001b[0;36mquestion_1d_sanity_check\u001b[0;34m(model, src_sents, tgt_sents, vocab)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0menc_hiddens_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_init_state_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_hiddens_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hiddens_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"enc_hiddens is incorrect: it should be:\\n {} but is:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_hiddens_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hiddens_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"enc_hiddens Sanity Checks Passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_init_state_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_init_state_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dec_init_state[0] is incorrect: it should be:\\n {} but is:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_init_state_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_init_state_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"]}]},{"cell_type":"code","metadata":{"id":"w0LvdskaisP3","colab_type":"code","colab":{}},"source":["import sys\n","\n","import numpy as np\n","\n","from docopt import docopt\n","from utils import batch_iter\n","from utils import read_corpus\n","from vocab import Vocab, VocabEntry\n","\n","#from nmt_model import NMT\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.utils\n","\n","#----------\n","# CONSTANTS\n","#----------\n","BATCH_SIZE = 5\n","EMBED_SIZE = 3\n","HIDDEN_SIZE = 3\n","DROPOUT_RATE = 0.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Xa0MFhF4yV7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594929590177,"user_tz":300,"elapsed":579,"user":{"displayName":"Subhodip Saha","photoUrl":"https://lh4.googleusercontent.com/-A4vfF5DeLH0/AAAAAAAAAAI/AAAAAAAAAh8/UijQ_Akhn3k/s64/photo.jpg","userId":"01295837312679332258"}},"outputId":"6db350f6-0380-4a1d-8316-d1e744132853"},"source":["#FOLDERNAME = 'AI/ManningNLP/a4'\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/sanity_check_en_es_data/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/AI/ManningNLP/a4/sanity_check_en_es_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vpCpD5akOF9E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594929739474,"user_tz":300,"elapsed":821,"user":{"displayName":"Subhodip Saha","photoUrl":"https://lh4.googleusercontent.com/-A4vfF5DeLH0/AAAAAAAAAAI/AAAAAAAAAh8/UijQ_Akhn3k/s64/photo.jpg","userId":"01295837312679332258"}},"outputId":"7c35b245-c5e1-4787-8b76-4358bf417ee7"},"source":["%cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/AI/ManningNLP/a4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-IK1HHJuMUG-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"ok","timestamp":1594929746269,"user_tz":300,"elapsed":2769,"user":{"displayName":"Subhodip Saha","photoUrl":"https://lh4.googleusercontent.com/-A4vfF5DeLH0/AAAAAAAAAAI/AAAAAAAAAh8/UijQ_Akhn3k/s64/photo.jpg","userId":"01295837312679332258"}},"outputId":"dcd9ff2a-9c49-4b90-d786-d1c7fde19132"},"source":["%ls -la"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 136\n","-rw------- 1 root root 54797 Jul 16 20:00 a4.ipynb\n","-rw------- 1 root root    98 Jan 29 00:07 collect_submission.sh\n","drwx------ 2 root root  4096 Jul 16 15:46 \u001b[0m\u001b[01;34men_es_data\u001b[0m/\n","-rw------- 1 root root    25 Jan 10  2020 gpu_requirements.txt\n","-rw------- 1 root root     0 Jan 29 00:07 __init__.py\n","-rw------- 1 root root   157 Jan 29 00:07 local_env.yml\n","-rw------- 1 root root  1889 Jan 29 00:34 model_embeddings.py\n","-rw------- 1 root root 25489 Jan 29 00:34 nmt_model.py\n","drwx------ 2 root root  4096 Jul 16 15:46 \u001b[01;34moutputs\u001b[0m/\n","drwx------ 2 root root  4096 Jul 16 19:47 \u001b[01;34m__pycache__\u001b[0m/\n","-rw------- 1 root root    95 Jan 10  2020 README.md\n","-rw------- 1 root root 14619 Jan 29 00:34 run.py\n","-rw------- 1 root root   911 Jan 29 00:07 run.sh\n","drwx------ 2 root root  4096 Jul 16 15:46 \u001b[01;34msanity_check_en_es_data\u001b[0m/\n","-rw------- 1 root root 10245 Jan 29 00:34 sanity_check.py\n","-rw------- 1 root root  2481 Jan 29 00:34 utils.py\n","-rw------- 1 root root  8155 Jan 29 00:34 vocab.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kuvzeKI8NlTd","colab_type":"code","colab":{}},"source":["!python  "],"execution_count":null,"outputs":[]}]}